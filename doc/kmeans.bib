
@book{bishop_pattern_2006,
	edition = {1},
	series = {Information science and statistics},
	title = {Pattern recognition and machine learning},
	isbn = {978-0-387-31073-2},
	abstract = {This is the first textbook on pattern recognition to present the Bayesian viewpoint. The book presents approximate inference algorithms that permit fast approximate answers in situations where exact answers are not feasible. It uses graphical models to describe probability distributions when no other books apply graphical models to machine learning. No previous knowledge of pattern recognition or machine learning concepts is assumed. Familiarity with multivariate calculus and basic linear algebra is required, and some experience in the use of probabilities would be helpful though not essential as the book includes a self-contained introduction to basic probability theory.},
	language = {en},
	publisher = {Springer},
	author = {Bishop, Christopher M.},
	month = aug,
	year = {2006},
	file = {Bishop - 2006 - Pattern recognition and machine learning.pdf:/Users/miguel/Library/Application Support/Zotero/Profiles/j358n6qi.default/zotero/storage/C5AUW85K/Bishop - 2006 - Pattern recognition and machine learning.pdf:application/pdf;prml-errata-3rd-20110921.pdf:/Users/miguel/Library/Application Support/Zotero/Profiles/j358n6qi.default/zotero/storage/RFZNA9GJ/prml-errata-3rd-20110921.pdf:application/pdf;prml-web-sol-2009-09-08.pdf:/Users/miguel/Library/Application Support/Zotero/Profiles/j358n6qi.default/zotero/storage/VCVMTHSA/prml-web-sol-2009-09-08.pdf:application/pdf}
}

@book{murphy_machine_2012,
	title = {Machine learning: a probabilistic perspective},
	isbn = {978-0-262-01802-9},
	shorttitle = {Machine {Learning}},
	url = {http://www.cs.ubc.ca/~murphyk/MLbook/},
	abstract = {Today's Web-enabled deluge of electronic data calls for automated methods of data analysis. Machine learning provides these, developing methods that can automatically detect patterns in data and then use the uncovered patterns to predict future data. This textbook offers a comprehensive and self-contained introduction to the field of machine learning, based on a unified, probabilistic approach. The coverage combines breadth and depth, offering necessary background material on such topics as probability, optimization, and linear algebra as well as discussion of recent developments in the field, including conditional random fields, L1 regularization, and deep learning. The book is written in an informal, accessible style, complete with pseudo-code for the most important algorithms. All topics are copiously illustrated with color images and worked examples drawn from such application domains as biology, text processing, computer vision, and robotics. Rather than providing a cookbook of different heuristic methods, the book stresses a principled model-based approach, often using the language of graphical models to specify models in a concise and intuitive way. Almost all the models described have been implemented in a MATLAB software package--PMTK (probabilistic modeling toolkit)--that is freely available online. The book is suitable for upper-level undergraduates with an introductory-level college math background and beginning graduate students.},
	language = {en},
	publisher = {MIT Press},
	author = {Murphy, Kevin P.},
	month = aug,
	year = {2012},
	file = {Murphy - 2012 - Machine Learning A Probabilistic Perspective.pdf:/Users/miguel/Library/Application Support/Zotero/Profiles/j358n6qi.default/zotero/storage/38NTNB8K/Murphy - 2012 - Machine Learning A Probabilistic Perspective.pdf:application/pdf}
}

@inproceedings{arthur_k-means++:_2007,
	address = {Philadelphia, PA, USA},
	series = {{SODA} '07},
	title = {K-means++: {The} {Advantages} of {Careful} {Seeding}},
	isbn = {978-0-89871-624-5},
	shorttitle = {K-means++},
	url = {http://dl.acm.org/citation.cfm?id=1283383.1283494},
	abstract = {The k-means method is a widely used clustering technique that seeks to minimize the average squared distance between points in the same cluster. Although it offers no accuracy guarantees, its simplicity and speed are very appealing in practice. By augmenting k-means with a very simple, randomized seeding technique, we obtain an algorithm that is Î˜(logk)-competitive with the optimal clustering. Preliminary experiments show that our augmentation improves both the speed and the accuracy of k-means, often quite dramatically.},
	urldate = {2016-04-27},
	booktitle = {Proceedings of the {Eighteenth} {Annual} {ACM}-{SIAM} {Symposium} on {Discrete} {Algorithms}},
	publisher = {Society for Industrial and Applied Mathematics},
	author = {Arthur, David and Vassilvitskii, Sergei},
	year = {2007},
	pages = {1027--1035},
	file = {Arthur and Vassilvitskii - 2007 - K-means++ The Advantages of Careful Seeding.pdf:/Users/miguel/Library/Application Support/Zotero/Profiles/j358n6qi.default/zotero/storage/66CG6PKR/Arthur and Vassilvitskii - 2007 - K-means++ The Advantages of Careful Seeding.pdf:application/pdf}
}